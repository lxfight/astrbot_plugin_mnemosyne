# -*- coding: utf-8 -*-
"""
Mnemosyne æ’ä»¶çš„å‘½ä»¤å¤„ç†å‡½æ•°å®ç°
(æ³¨æ„ï¼šè£…é¥°å™¨å·²ç§»é™¤ï¼Œå‡½æ•°æ¥æ”¶ self)
"""

from typing import TYPE_CHECKING, Optional
from datetime import datetime
import asyncio

# å¯¼å…¥ AstrBot API å’Œç±»å‹ (ä»…éœ€è¦äº‹ä»¶å’Œæ¶ˆæ¯æ®µ)
from astrbot.api.event import AstrMessageEvent

# å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œå¸¸é‡
from .constants import PRIMARY_FIELD_NAME, MAX_TOTAL_FETCH_RECORDS

# å¯¼å…¥è¿ç§»ç›¸å…³æ¨¡å—
from ..memory_manager.vector_db import VectorDatabaseFactory
from ..memory_manager.embedding_adapter import EmbeddingServiceFactory

# ç±»å‹æç¤º
if TYPE_CHECKING:
    from ..main import Mnemosyne


async def list_collections_cmd_impl(self: "Mnemosyne", event: AstrMessageEvent):
    """[å®ç°] åˆ—å‡ºå½“å‰å‘é‡æ•°æ®åº“å®ä¾‹ä¸­çš„æ‰€æœ‰é›†åˆ"""
    if not self.vector_db or not self.vector_db.is_connected():
        db_type = self.vector_db.get_database_type().value if self.vector_db else "å‘é‡æ•°æ®åº“"
        yield event.plain_result(f"âš ï¸ {db_type} æœåŠ¡æœªåˆå§‹åŒ–æˆ–æœªè¿æ¥ã€‚")
        return
    try:
        collections = self.vector_db.list_collections()
        if collections is None:
            yield event.plain_result("âš ï¸ è·å–é›†åˆåˆ—è¡¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
            return
        if not collections:
            db_type = self.vector_db.get_database_type().value
            response = f"å½“å‰ {db_type} å®ä¾‹ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é›†åˆã€‚"
        else:
            db_type = self.vector_db.get_database_type().value
            response = f"å½“å‰ {db_type} å®ä¾‹ä¸­çš„é›†åˆåˆ—è¡¨ï¼š\n" + "\n".join(
                [f"ğŸ“š {col}" for col in collections]
            )
            if self.collection_name in collections:
                response += f"\n\nå½“å‰æ’ä»¶ä½¿ç”¨çš„é›†åˆ: {self.collection_name}"
            else:
                response += (
                    f"\n\nâš ï¸ å½“å‰æ’ä»¶é…ç½®çš„é›†åˆ '{self.collection_name}' ä¸åœ¨åˆ—è¡¨ä¸­ï¼"
                )
        yield event.plain_result(response)
    except Exception as e:
        self.logger.error(f"æ‰§è¡Œ 'memory list' å‘½ä»¤å¤±è´¥: {str(e)}", exc_info=True)
        yield event.plain_result(f"âš ï¸ è·å–é›†åˆåˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")


async def delete_collection_cmd_impl(
    self: "Mnemosyne",
    event: AstrMessageEvent,
    collection_name: str,
    confirm: Optional[str] = None,
):
    """[å®ç°] åˆ é™¤æŒ‡å®šçš„å‘é‡æ•°æ®åº“é›†åˆåŠå…¶æ‰€æœ‰æ•°æ®"""
    if not self.vector_db or not self.vector_db.is_connected():
        db_type = self.vector_db.get_database_type().value if self.vector_db else "å‘é‡æ•°æ®åº“"
        yield event.plain_result(f"âš ï¸ {db_type} æœåŠ¡æœªåˆå§‹åŒ–æˆ–æœªè¿æ¥ã€‚")
        return

    is_current_collection = collection_name == self.collection_name
    warning_msg = ""
    if is_current_collection:
        warning_msg = f"\n\nğŸ”¥ğŸ”¥ğŸ”¥ è­¦å‘Šï¼šæ‚¨æ­£åœ¨å°è¯•åˆ é™¤å½“å‰æ’ä»¶æ­£åœ¨ä½¿ç”¨çš„é›†åˆ '{collection_name}'ï¼è¿™å°†å¯¼è‡´æ’ä»¶åŠŸèƒ½å¼‚å¸¸ï¼Œç›´åˆ°é‡æ–°åˆ›å»ºæˆ–æ›´æ”¹é…ç½®ï¼ ğŸ”¥ğŸ”¥ğŸ”¥"

    db_type = self.vector_db.get_database_type().value
    if confirm != "--confirm":
        yield event.plain_result(
            f"âš ï¸ æ“ä½œç¡®è®¤ âš ï¸\n"
            f"æ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤ {db_type} é›†åˆ '{collection_name}' åŠå…¶åŒ…å«çš„æ‰€æœ‰æ•°æ®ï¼æ­¤æ“ä½œæ— æ³•æ’¤é”€ï¼\n"
            f"{warning_msg}\n\n"
            f"å¦‚æœæ‚¨ç¡®å®šè¦ç»§ç»­ï¼Œè¯·å†æ¬¡æ‰§è¡Œå‘½ä»¤å¹¶æ·»åŠ  `--confirm` å‚æ•°:\n"
            f"`/memory drop_collection {collection_name} --confirm`"
        )
        return

    try:
        sender_id = event.get_sender_id()
        self.logger.warning(
            f"ç®¡ç†å‘˜ {sender_id} è¯·æ±‚åˆ é™¤é›†åˆ: {collection_name} (ç¡®è®¤æ‰§è¡Œ)"
        )
        if is_current_collection:
            self.logger.critical(
                f"ç®¡ç†å‘˜ {sender_id} æ­£åœ¨åˆ é™¤å½“å‰æ’ä»¶ä½¿ç”¨çš„é›†åˆ '{collection_name}'ï¼"
            )

        success = self.vector_db.drop_collection(collection_name)
        if success:
            msg = f"âœ… å·²æˆåŠŸåˆ é™¤ {db_type} é›†åˆ '{collection_name}'ã€‚"
            if is_current_collection:
                msg += "\næ’ä»¶ä½¿ç”¨çš„é›†åˆå·²è¢«åˆ é™¤ï¼Œè¯·å°½å¿«å¤„ç†ï¼"
            yield event.plain_result(msg)
            self.logger.warning(f"ç®¡ç†å‘˜ {sender_id} æˆåŠŸåˆ é™¤äº†é›†åˆ: {collection_name}")
            if is_current_collection:
                self.logger.error(
                    f"æ’ä»¶å½“å‰ä½¿ç”¨çš„é›†åˆ '{collection_name}' å·²è¢«åˆ é™¤ï¼Œç›¸å…³åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚"
                )
        else:
            yield event.plain_result(
                f"âš ï¸ åˆ é™¤é›†åˆ '{collection_name}' çš„è¯·æ±‚å·²å‘é€ï¼Œä½† {db_type} è¿”å›å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
            )

    except Exception as e:
        self.logger.error(
            f"æ‰§è¡Œ 'memory drop_collection {collection_name}' å‘½ä»¤æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}",
            exc_info=True,
        )
        yield event.plain_result(f"âš ï¸ åˆ é™¤é›†åˆæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")


async def list_records_cmd_impl(
    self: "Mnemosyne",
    event: AstrMessageEvent,
    collection_name: Optional[str] = None,
    limit: int = 5,
):
    """[å®ç°] æŸ¥è¯¢æŒ‡å®šé›†åˆçš„æœ€æ–°è®°å¿†è®°å½• (æŒ‰åˆ›å»ºæ—¶é—´å€’åºï¼Œè‡ªåŠ¨è·å–æœ€æ–°)"""
    if not self.vector_db or not self.vector_db.is_connected():
        db_type = self.vector_db.get_database_type().value if self.vector_db else "å‘é‡æ•°æ®åº“"
        yield event.plain_result(f"âš ï¸ {db_type} æœåŠ¡æœªåˆå§‹åŒ–æˆ–æœªè¿æ¥ã€‚")
        return

    # è·å–å½“å‰ä¼šè¯çš„ session_id (å¦‚æœéœ€è¦æŒ‰ä¼šè¯è¿‡æ»¤)
    session_id = await self.context.conversation_manager.get_curr_conversation_id(
        event.unified_msg_origin
    )
    # session_id = "session_1" # å¦‚æœè¦æµ‹è¯•ç‰¹å®šä¼šè¯æˆ–æ— ä¼šè¯è¿‡æ»¤ï¼Œå¯ä»¥åœ¨è¿™é‡Œç¡¬ç¼–ç æˆ–è®¾ä¸º None

    target_collection = collection_name or self.collection_name

    # å¯¹ç”¨æˆ·è¾“å…¥çš„ limit è¿›è¡ŒéªŒè¯
    if limit <= 0 or limit > 50:
        # é™åˆ¶ç”¨æˆ·è¯·æ±‚çš„æ˜¾ç¤ºæ•°é‡
        yield event.plain_result("âš ï¸ æ˜¾ç¤ºæ•°é‡ (limit) å¿…é¡»åœ¨ 1 åˆ° 50 ä¹‹é—´ã€‚")
        return

    try:
        if not self.vector_db.has_collection(target_collection):
            yield event.plain_result(f"âš ï¸ é›†åˆ '{target_collection}' ä¸å­˜åœ¨ã€‚")
            return

        # æ„å»ºæŸ¥è¯¢è¡¨è¾¾å¼ - ä»…åŸºäº session_id (å¦‚æœéœ€è¦)
        if session_id:
            # å¦‚æœæœ‰ä¼šè¯IDï¼Œåˆ™æŒ‰ä¼šè¯IDè¿‡æ»¤
            expr = f'session_id in ["{session_id}"]'
            self.logger.info(
                f"å°†æŒ‰ä¼šè¯ ID '{session_id}' è¿‡æ»¤å¹¶æŸ¥è¯¢æ‰€æœ‰ç›¸å…³è®°å½• (ä¸Šé™ {MAX_TOTAL_FETCH_RECORDS} æ¡)ã€‚"
            )
        else:
            # å¦‚æœæ²¡æœ‰ä¼šè¯IDä¸Šä¸‹æ–‡ï¼ŒæŸ¥è¯¢æ‰€æœ‰è®°å½•
            expr = f"{PRIMARY_FIELD_NAME} >= 0"
            self.logger.info(
                "æœªæŒ‡å®šä¼šè¯ IDï¼Œå°†æŸ¥è¯¢é›†åˆ '{target_collection}' ä¸­çš„æ‰€æœ‰è®°å½• (ä¸Šé™ {MAX_TOTAL_FETCH_RECORDS} æ¡)ã€‚"
            )
            # æˆ–è€…ï¼Œå¦‚æœæ‚¨çš„ milvus_manager æ”¯æŒç©ºè¡¨è¾¾å¼æŸ¥è¯¢æ‰€æœ‰ï¼Œåˆ™ expr = "" æˆ– None

        # self.logger.debug(f"æŸ¥è¯¢é›†åˆ '{target_collection}' è®°å½•: expr='{expr}'") # ä¸Šé¢å·²æœ‰æ›´å…·ä½“çš„æ—¥å¿—
        output_fields = [
            "content",
            "create_time",
            "session_id",
            "personality_id",
            PRIMARY_FIELD_NAME,
        ]

        self.logger.debug(
            f"å‡†å¤‡æŸ¥è¯¢ Milvus: é›†åˆ='{target_collection}', è¡¨è¾¾å¼='{expr}', é™åˆ¶={limit},è¾“å‡ºå­—æ®µ={output_fields}, æ€»æ•°ä¸Šé™={MAX_TOTAL_FETCH_RECORDS}"
        )

        # ç›´æ¥ä½¿ç”¨ Milvus çš„ offset å’Œ limit å‚æ•°è¿›è¡Œåˆ†é¡µæŸ¥è¯¢
        # records = self.milvus_manager.query(
        #     collection_name=target_collection,
        #     expression=expr,
        #     output_fields=output_fields,
        #     limit=limit,
        #     offset=offset,  # ç›´æ¥ä½¿ç”¨å‡½æ•°å‚æ•° offset
        # )

        # é‡è¦çš„ä¿®æ”¹ï¼šç§»é™¤å‘é‡æ•°æ®åº“ query çš„ offset å’Œ limit å‚æ•°ï¼Œä½¿ç”¨æ€»æ•°ä¸Šé™ä½œä¸º limit
        fetched_records = self.vector_db.query(
            collection_name=target_collection,
            expression=expr,
            output_fields=output_fields,
            limit=MAX_TOTAL_FETCH_RECORDS,  # ä½¿ç”¨æ€»æ•°ä¸Šé™ä½œä¸ºå‘é‡æ•°æ®åº“çš„ limit
        )

        # æ£€æŸ¥æŸ¥è¯¢ç»“æœ
        if fetched_records is None:
            # æŸ¥è¯¢å¤±è´¥ï¼Œvector_db.query é€šå¸¸ä¼šè¿”å› None æˆ–æŠ›å‡ºå¼‚å¸¸
            self.logger.error(
                f"æŸ¥è¯¢é›†åˆ '{target_collection}' å¤±è´¥ï¼Œvector_db.query è¿”å› Noneã€‚"
            )
            yield event.plain_result(
                f"âš ï¸ æŸ¥è¯¢é›†åˆ '{target_collection}' è®°å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚"
            )
            return

        if not fetched_records:
            # æŸ¥è¯¢æˆåŠŸï¼Œä½†æ²¡æœ‰è¿”å›ä»»ä½•è®°å½•
            session_filter_msg = f"åœ¨ä¼šè¯ '{session_id}' ä¸­" if session_id else ""
            self.logger.info(
                f"é›†åˆ '{target_collection}' {session_filter_msg} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„è®°å¿†è®°å½•ã€‚"
            )
            yield event.plain_result(
                f"é›†åˆ '{target_collection}' {session_filter_msg} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„è®°å¿†è®°å½•ã€‚"
            )
            return
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº†æ€»æ•°ä¸Šé™
        if len(fetched_records) >= MAX_TOTAL_FETCH_RECORDS:
            self.logger.warning(
                f"æŸ¥è¯¢åˆ°çš„è®°å½•æ•°é‡è¾¾åˆ°æ€»æ•°ä¸Šé™ ({MAX_TOTAL_FETCH_RECORDS})ï¼Œå¯èƒ½å­˜åœ¨æ›´å¤šæœªè·å–çš„è®°å½•ï¼Œå¯¼è‡´æ— æ³•æ‰¾åˆ°æ›´æ—§çš„è®°å½•ï¼Œä½†æœ€æ–°è®°å½•åº”è¯¥åœ¨è·å–èŒƒå›´å†…ã€‚"
            )
            yield event.plain_result(
                f"â„¹ï¸ è­¦å‘Šï¼šæŸ¥è¯¢åˆ°çš„è®°å½•æ•°é‡å·²è¾¾åˆ°ç³»ç»Ÿè·å–æœ€æ–°è®°å½•çš„ä¸Šé™ ({MAX_TOTAL_FETCH_RECORDS})ã€‚å¦‚æœè®°å½•éå¸¸å¤šï¼Œå¯èƒ½æ— æ³•æ˜¾ç¤ºæ›´æ—§çš„å†…å®¹ï¼Œä½†æœ€æ–°è®°å½•åº”è¯¥å·²åŒ…å«åœ¨å†…ã€‚"
            )

        self.logger.debug(f"æˆåŠŸè·å–åˆ° {len(fetched_records)} æ¡åŸå§‹è®°å½•ç”¨äºæ’åºã€‚")
        # --- åœ¨è·å–å…¨éƒ¨ç»“æœåè¿›è¡Œæ’åº (æŒ‰åˆ›å»ºæ—¶é—´å€’åº) ---
        # è¿™ç¡®ä¿äº†æ’åºæ˜¯åŸºäºæ‰€æœ‰è·å–åˆ°çš„è®°å½•ï¼Œæ‰¾åˆ°çœŸæ­£çš„æœ€æ–°è®°å½•
        try:
            # ä½¿ç”¨ lambda è¡¨è¾¾å¼æŒ‰ create_time å­—æ®µæ’åºï¼Œå¦‚æœå­—æ®µä¸å­˜åœ¨æˆ–ä¸º Noneï¼Œé»˜è®¤ä¸º 0
            fetched_records.sort(
                key=lambda x: x.get("create_time", 0) or 0, reverse=True
            )
            self.logger.debug(
                f"å·²å°†è·å–åˆ°çš„ {len(fetched_records)} æ¡è®°å½•æŒ‰ create_time é™åºæ’åºã€‚"
            )
        except Exception as sort_e:
            self.logger.warning(
                f"å¯¹æŸ¥è¯¢ç»“æœè¿›è¡Œæ’åºæ—¶å‡ºé”™: {sort_e}ã€‚æ˜¾ç¤ºé¡ºåºå¯èƒ½ä¸æŒ‰æ—¶é—´æ’åºã€‚"
            )
            # å¦‚æœæ’åºå¤±è´¥ï¼Œç»§ç»­å¤„ç†ï¼Œä½†ä¸ä¿è¯æŒ‰æ—¶é—´é¡ºåº

        # --- åœ¨æ’åºåè·å–æœ€å‰çš„ limit æ¡è®°å½• ---
        # ä»æ’åºåçš„ fetched_records ä¸­å–å‡ºæœ€å‰çš„ limit æ¡è®°å½•
        display_records = fetched_records[:limit]

        # display_records ä¸ä¼šä¸ºç©ºï¼Œé™¤é fetched_records æœ¬èº«å°±ä¸ºç©ºï¼Œ
        # è€Œ fetched_records ä¸ºç©ºçš„æƒ…å†µå·²ç»åœ¨å‰é¢å¤„ç†è¿‡äº†ã€‚

        # å‡†å¤‡å“åº”æ¶ˆæ¯
        total_fetched = len(fetched_records)
        display_count = len(display_records)
        # æ¶ˆæ¯æç¤ºç”¨æˆ·è¿™æ˜¯æœ€æ–°çš„è®°å½•
        response_lines = [
            f"ğŸ“œ é›†åˆ '{target_collection}' çš„æœ€æ–°è®°å¿†è®°å½• (å…±è·å– {total_fetched} æ¡è¿›è¡Œæ’åº, æ˜¾ç¤ºæœ€æ–°çš„ {display_count} æ¡):"
        ]

        # æ ¼å¼åŒ–æ¯æ¡è®°å½•ä»¥ä¾›æ˜¾ç¤º
        # ä½¿ç”¨ enumerate ä» 1 å¼€å§‹ç”Ÿæˆåºå·
        for i, record in enumerate(display_records, start=1):
            ts = record.get("create_time")
            try:
                # æ ¹æ® Milvus æ–‡æ¡£ï¼ŒQuery ç»“æœä¸­çš„ time æ˜¯ float ç±»å‹çš„ Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ã€‚
                time_str = (
                    datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")
                    if ts is not None  # æ£€æŸ¥ ts æ˜¯å¦å­˜åœ¨ä¸”ä¸æ˜¯ None
                    else "æœªçŸ¥æ—¶é—´"
                )
            except (TypeError, ValueError, OSError) as time_e:
                # å¤„ç†æ— æ•ˆæˆ–æ— æ³•è§£æçš„æ—¶é—´æˆ³
                self.logger.warning(
                    f"è®°å½• {record.get(PRIMARY_FIELD_NAME, 'æœªçŸ¥ID')} çš„æ—¶é—´æˆ³ '{ts}' æ— æ•ˆæˆ–è§£æé”™è¯¯: {time_e}"
                )
                time_str = f"æ— æ•ˆæ—¶é—´æˆ³({ts})" if ts is not None else "æœªçŸ¥æ—¶é—´"

            content = record.get("content", "å†…å®¹ä¸å¯ç”¨")
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹ä»¥ä¼˜åŒ–æ˜¾ç¤º
            content_preview = content[:200] + ("..." if len(content) > 200 else "")
            record_session_id = record.get("session_id", "æœªçŸ¥ä¼šè¯")
            persona_id = record.get("personality_id", "æœªçŸ¥äººæ ¼")
            pk = record.get(PRIMARY_FIELD_NAME, "æœªçŸ¥ID")  # è·å–ä¸»é”®

            response_lines.append(
                f"#{i} [ID: {pk}]\n"  # ä½¿ç”¨ä» 1 å¼€å§‹çš„åºå·
                f"  æ—¶é—´: {time_str}\n"
                f"  äººæ ¼: {persona_id}\n"
                f"  ä¼šè¯: {record_session_id}\n"
                f"  å†…å®¹: {content_preview}"
            )

        # å‘é€æ ¼å¼åŒ–åçš„ç»“æœ
        yield event.plain_result("\n\n".join(response_lines))

    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–æ½œåœ¨å¼‚å¸¸
        self.logger.error(
            f"æ‰§è¡Œ 'memory list_records' å‘½ä»¤æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ (é›†åˆ: {target_collection}): {str(e)}",
            exc_info=True,  # è®°å½•å®Œæ•´çš„é”™è¯¯å †æ ˆ
        )
        yield event.plain_result("âš ï¸ æŸ¥è¯¢è®°å¿†è®°å½•æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")


async def delete_session_memory_cmd_impl(
    self: "Mnemosyne",
    event: AstrMessageEvent,
    session_id: str,
    confirm: Optional[str] = None,
):
    """[å®ç°] åˆ é™¤æŒ‡å®šä¼šè¯ ID ç›¸å…³çš„æ‰€æœ‰è®°å¿†ä¿¡æ¯"""
    if not self.vector_db or not self.vector_db.is_connected():
        db_type = self.vector_db.get_database_type().value if self.vector_db else "å‘é‡æ•°æ®åº“"
        yield event.plain_result(f"âš ï¸ {db_type} æœåŠ¡æœªåˆå§‹åŒ–æˆ–æœªè¿æ¥ã€‚")
        return

    if not session_id or not session_id.strip():
        yield event.plain_result("âš ï¸ è¯·æä¾›è¦åˆ é™¤è®°å¿†çš„ä¼šè¯ ID (session_id)ã€‚")
        return

    session_id_to_delete = session_id.strip().strip('"`')

    if confirm != "--confirm":
        yield event.plain_result(
            f"âš ï¸ æ“ä½œç¡®è®¤ âš ï¸\n"
            f"æ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤ä¼šè¯ ID '{session_id_to_delete}' åœ¨é›†åˆ '{self.collection_name}' ä¸­çš„æ‰€æœ‰è®°å¿†ä¿¡æ¯ï¼æ­¤æ“ä½œæ— æ³•æ’¤é”€ï¼\n\n"
            f"è¦ç¡®è®¤åˆ é™¤ï¼Œè¯·å†æ¬¡æ‰§è¡Œå‘½ä»¤å¹¶æ·»åŠ  `--confirm` å‚æ•°:\n"
            f'`/memory delete_session_memory "{session_id_to_delete}" --confirm`'
        )
        return

    try:
        collection_name = self.collection_name
        expr = f'session_id == "{session_id_to_delete}"'
        sender_id = event.get_sender_id()
        self.logger.warning(
            f"ç®¡ç†å‘˜ {sender_id} è¯·æ±‚åˆ é™¤ä¼šè¯ '{session_id_to_delete}' çš„æ‰€æœ‰è®°å¿† (é›†åˆ: {collection_name}, è¡¨è¾¾å¼: '{expr}') (ç¡®è®¤æ‰§è¡Œ)"
        )

        mutation_result = self.vector_db.delete(
            collection_name=collection_name, expression=expr
        )

        if mutation_result:
            delete_pk_count = (
                mutation_result.delete_count
                if hasattr(mutation_result, "delete_count")
                else "æœªçŸ¥"
            )
            self.logger.info(
                f"å·²å‘é€åˆ é™¤ä¼šè¯ '{session_id_to_delete}' è®°å¿†çš„è¯·æ±‚ã€‚è¿”å›çš„åˆ é™¤è®¡æ•°ï¼ˆå¯èƒ½ä¸å‡†ç¡®ï¼‰: {delete_pk_count}"
            )
            try:
                self.logger.info(
                    f"æ­£åœ¨åˆ·æ–°é›†åˆ '{collection_name}' ä»¥åº”ç”¨åˆ é™¤æ“ä½œ..."
                )
                # å¯¹äº FAISSï¼Œflush æ“ä½œå¯èƒ½ä¸éœ€è¦ï¼Œä½†ä¿æŒæ¥å£ä¸€è‡´æ€§
                if hasattr(self.vector_db, 'flush'):
                    self.vector_db.flush([collection_name])
                self.logger.info(f"é›†åˆ '{collection_name}' åˆ·æ–°å®Œæˆã€‚åˆ é™¤æ“ä½œå·²ç”Ÿæ•ˆã€‚")
                yield event.plain_result(
                    f"âœ… å·²æˆåŠŸåˆ é™¤ä¼šè¯ ID '{session_id_to_delete}' çš„æ‰€æœ‰è®°å¿†ä¿¡æ¯ã€‚"
                )
            except Exception as flush_err:
                self.logger.error(
                    f"åˆ·æ–°é›†åˆ '{collection_name}' ä»¥åº”ç”¨åˆ é™¤æ—¶å‡ºé”™: {flush_err}",
                    exc_info=True,
                )
                yield event.plain_result(
                    f"âš ï¸ å·²å‘é€åˆ é™¤è¯·æ±‚ï¼Œä½†åœ¨åˆ·æ–°é›†åˆä½¿æ›´æ”¹ç”Ÿæ•ˆæ—¶å‡ºé”™: {flush_err}ã€‚åˆ é™¤å¯èƒ½æœªå®Œå…¨ç”Ÿæ•ˆã€‚"
                )
        else:
            db_type = self.vector_db.get_database_type().value
            yield event.plain_result(
                f"âš ï¸ åˆ é™¤ä¼šè¯ ID '{session_id_to_delete}' è®°å¿†çš„è¯·æ±‚å¤±è´¥ã€‚è¯·æ£€æŸ¥ {db_type} æ—¥å¿—ã€‚"
            )

    except Exception as e:
        self.logger.error(
            f"æ‰§è¡Œ 'memory delete_session_memory' å‘½ä»¤æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ (Session ID: {session_id_to_delete}): {str(e)}",
            exc_info=True,
        )
        yield event.plain_result(f"âš ï¸ åˆ é™¤ä¼šè¯è®°å¿†æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")


async def get_session_id_cmd_impl(self: "Mnemosyne", event: AstrMessageEvent):
    """[å®ç°] è·å–å½“å‰ä¸æ‚¨å¯¹è¯çš„ä¼šè¯ ID"""
    try:
        session_id = await self.context.conversation_manager.get_curr_conversation_id(
            event.unified_msg_origin
        )
        if session_id:
            yield event.plain_result(f"å½“å‰ä¼šè¯ ID: {session_id}")
        else:
            yield event.plain_result(
                "ğŸ¤” æ— æ³•è·å–å½“å‰ä¼šè¯ IDã€‚å¯èƒ½è¿˜æ²¡æœ‰å¼€å§‹å¯¹è¯ï¼Œæˆ–è€…ä¼šè¯å·²ç»“æŸ/å¤±æ•ˆã€‚"
            )
            self.logger.warning(
                f"ç”¨æˆ· {event.get_sender_id()} åœ¨ {event.unified_msg_origin} å°è¯•è·å– session_id å¤±è´¥ã€‚"
            )
    except Exception as e:
        self.logger.error(
            f"æ‰§è¡Œ 'memory get_session_id' å‘½ä»¤å¤±è´¥: {str(e)}", exc_info=True
        )
        yield event.plain_result(f"âš ï¸ è·å–å½“å‰ä¼šè¯ ID æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


# === è¿ç§»ç›¸å…³å‘½ä»¤å®ç° ===


async def migration_status_cmd_impl(self: "Mnemosyne", event: AstrMessageEvent):
    """[å®ç°] æŸ¥çœ‹å½“å‰æ’ä»¶é…ç½®å’Œè¿ç§»çŠ¶æ€"""
    try:
        # è·å–å½“å‰é…ç½®ä¿¡æ¯
        current_db_type = self.config.get("vector_database_type", "milvus")
        embedding_provider_id = self.config.get("embedding_provider_id", "")

        # æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€
        db_status = "âŒ æœªè¿æ¥"
        db_info = ""
        if self.vector_db:
            if self.vector_db.is_connected():
                db_status = "âœ… å·²è¿æ¥"
                stats = self.vector_db.get_collection_stats(self.collection_name)
                if stats:
                    db_info = f"\n  é›†åˆ: {self.collection_name}\n  è®°å½•æ•°: {stats.get('record_count', 0)}\n  å‘é‡ç»´åº¦: {stats.get('vector_dim', 'N/A')}"
            else:
                db_status = "âš ï¸ å·²åˆå§‹åŒ–ä½†æœªè¿æ¥"

        # æ£€æŸ¥åµŒå…¥æœåŠ¡çŠ¶æ€
        embedding_status = "âŒ æœªåˆå§‹åŒ–"
        embedding_info = ""
        if self.embedding_adapter:
            embedding_status = "âœ… å·²åˆå§‹åŒ–"
            embedding_info = f"\n  æœåŠ¡: {self.embedding_adapter.service_name}\n  æ¨¡å‹: {self.embedding_adapter.get_model_name()}\n  ç»´åº¦: {self.embedding_adapter.get_dim()}"

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°ç‰ˆæœ¬é…ç½®
        migration_version = self.config.get("_migration_version", "")
        is_migrated = "âœ… å·²è¿ç§»åˆ° v0.6.0" if migration_version else "âš ï¸ å¯èƒ½éœ€è¦è¿ç§»"

        response = f"""ğŸ“Š Mnemosyne æ’ä»¶çŠ¶æ€æŠ¥å‘Š

ğŸ”§ é…ç½®ä¿¡æ¯:
  ç‰ˆæœ¬: v0.6.0
  æ•°æ®åº“ç±»å‹: {current_db_type}
  åµŒå…¥æœåŠ¡ID: {embedding_provider_id or "ä½¿ç”¨ä¼ ç»Ÿé…ç½®"}
  è¿ç§»çŠ¶æ€: {is_migrated}

ğŸ’¾ æ•°æ®åº“çŠ¶æ€: {db_status}{db_info}

ğŸ¤– åµŒå…¥æœåŠ¡çŠ¶æ€: {embedding_status}{embedding_info}

ğŸ“ å¯ç”¨è¿ç§»å‘½ä»¤:
  /memory migrate_config - è¿ç§»é…ç½®åˆ°æ–°æ ¼å¼
  /memory migrate_to_faiss - è¿ç§»åˆ° FAISS æ•°æ®åº“
  /memory migrate_to_milvus - è¿ç§»åˆ° Milvus æ•°æ®åº“
  /memory validate_config - éªŒè¯å½“å‰é…ç½®"""

        yield event.plain_result(response)

    except Exception as e:
        self.logger.error(f"è·å–è¿ç§»çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
        yield event.plain_result(f"âš ï¸ è·å–çŠ¶æ€ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


async def migrate_config_cmd_impl(self: "Mnemosyne", event: AstrMessageEvent):
    """[å®ç°] è¿ç§»é…ç½®åˆ°æ–°æ ¼å¼"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»è¿ç§»
        if self.config.get("_migration_version"):
            yield event.plain_result("âœ… é…ç½®å·²ç»æ˜¯æ–°æ ¼å¼ï¼Œæ— éœ€è¿ç§»ã€‚")
            return

        yield event.plain_result("ğŸ”„ å¼€å§‹è¿ç§»é…ç½®åˆ°æ–°æ ¼å¼...")

        # æ·»åŠ æ–°çš„é…ç½®é¡¹
        if "vector_database_type" not in self.config:
            # æ ¹æ®ç°æœ‰é…ç½®åˆ¤æ–­æ•°æ®åº“ç±»å‹
            if self.config.get("milvus_lite_path") or self.config.get("address"):
                self.config["vector_database_type"] = "milvus"
                yield event.plain_result(
                    "âœ“ æ£€æµ‹åˆ° Milvus é…ç½®ï¼Œè®¾ç½®æ•°æ®åº“ç±»å‹ä¸º milvus"
                )
            else:
                self.config["vector_database_type"] = "faiss"
                yield event.plain_result(
                    "âœ“ æœªæ£€æµ‹åˆ° Milvus é…ç½®ï¼Œè®¾ç½®æ•°æ®åº“ç±»å‹ä¸º faiss"
                )

        # æ·»åŠ  FAISS é»˜è®¤é…ç½®
        if "faiss_config" not in self.config:
            self.config["faiss_config"] = {}

        faiss_config = self.config["faiss_config"]
        if "faiss_data_path" not in faiss_config:
            faiss_config["faiss_data_path"] = "faiss_data"
        if "faiss_index_type" not in faiss_config:
            faiss_config["faiss_index_type"] = "IndexFlatL2"
        if "faiss_nlist" not in faiss_config:
            faiss_config["faiss_nlist"] = 100

        # æ·»åŠ åµŒå…¥æœåŠ¡æä¾›å•†IDé…ç½®
        if "embedding_provider_id" not in self.config:
            self.config["embedding_provider_id"] = ""

        # æ ‡è®°è¿ç§»ç‰ˆæœ¬
        self.config["_migration_version"] = "0.6.0"
        self.config["_migration_date"] = datetime.now().isoformat()

        yield event.plain_result("âœ… é…ç½®è¿ç§»å®Œæˆï¼æ–°å¢é…ç½®é¡¹ï¼š")
        yield event.plain_result(
            f"  - vector_database_type: {self.config['vector_database_type']}"
        )
        yield event.plain_result(
            f"  - faiss_config.faiss_data_path: {self.config['faiss_config']['faiss_data_path']}"
        )
        yield event.plain_result(
            f"  - faiss_config.faiss_index_type: {self.config['faiss_config']['faiss_index_type']}"
        )
        yield event.plain_result(
            f"  - embedding_provider_id: {self.config['embedding_provider_id']}"
        )
        yield event.plain_result("\nâš ï¸ æ³¨æ„ï¼šé…ç½®å·²æ›´æ–°ï¼Œå»ºè®®é‡å¯æ’ä»¶ä»¥åº”ç”¨æ›´æ”¹ã€‚")

    except Exception as e:
        self.logger.error(f"é…ç½®è¿ç§»å¤±è´¥: {e}", exc_info=True)
        yield event.plain_result(f"âš ï¸ é…ç½®è¿ç§»å¤±è´¥: {str(e)}")


async def migrate_to_faiss_cmd_impl(
    self: "Mnemosyne", event: AstrMessageEvent, confirm: Optional[str] = None
):
    """[å®ç°] è¿ç§»æ•°æ®åˆ° FAISS æ•°æ®åº“"""
    current_db_type = self.config.get("vector_database_type", "milvus")

    if current_db_type == "faiss":
        yield event.plain_result("âœ… å½“å‰å·²ç»ä½¿ç”¨ FAISS æ•°æ®åº“ï¼Œæ— éœ€è¿ç§»ã€‚")
        return

    if confirm != "--confirm":
        yield event.plain_result(
            f"âš ï¸ æ•°æ®åº“è¿ç§»ç¡®è®¤ âš ï¸\n"
            f"æ­¤æ“ä½œå°†æŠŠæ•°æ®ä» {current_db_type} è¿ç§»åˆ° FAISS æ•°æ®åº“ã€‚\n"
            f"è¿ç§»è¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·ç¡®ä¿ï¼š\n"
            f"1. å½“å‰æ•°æ®åº“è¿æ¥æ­£å¸¸\n"
            f"2. æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´\n"
            f"3. è¿ç§»æœŸé—´é¿å…å…¶ä»–æ“ä½œ\n\n"
            f"å¦‚æœç¡®è®¤è¿ç§»ï¼Œè¯·æ‰§è¡Œï¼š\n"
            f"/memory migrate_to_faiss --confirm"
        )
        return

    try:
        yield event.plain_result("ğŸ”„ å¼€å§‹è¿ç§»åˆ° FAISS æ•°æ®åº“...")

        # æ£€æŸ¥å½“å‰æ•°æ®åº“è¿æ¥
        if not self.vector_db or not self.vector_db.is_connected():
            yield event.plain_result("âŒ å½“å‰æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•è¿›è¡Œè¿ç§»ã€‚")
            return

        # åˆ›å»º FAISS æ•°æ®åº“é…ç½®
        current_faiss_config = self.config.get("faiss_config", {})
        faiss_config = {
            "faiss_config": {
                "faiss_data_path": current_faiss_config.get("faiss_data_path", "faiss_data"),
                "faiss_index_type": current_faiss_config.get("faiss_index_type", "IndexFlatL2"),
                "faiss_nlist": current_faiss_config.get("faiss_nlist", 100),
            }
        }

        # åˆ›å»ºç›®æ ‡ FAISS æ•°æ®åº“
        yield event.plain_result("ğŸ“¦ åˆ›å»º FAISS æ•°æ®åº“å®ä¾‹...")
        target_db = VectorDatabaseFactory.create_database("faiss", faiss_config)
        if not target_db or not target_db.connect():
            yield event.plain_result("âŒ æ— æ³•åˆ›å»ºæˆ–è¿æ¥åˆ° FAISS æ•°æ®åº“ã€‚")
            return

        # æ‰§è¡Œæ•°æ®è¿ç§»
        yield event.plain_result(f"ğŸ“‹ å¼€å§‹è¿ç§»é›†åˆ '{self.collection_name}' çš„æ•°æ®...")

        # åœ¨åå°æ‰§è¡Œè¿ç§»
        success = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: VectorDatabaseFactory.migrate_data(
                source_db=self.vector_db,
                target_db=target_db,
                collection_name=self.collection_name,
                batch_size=1000,
            ),
        )

        if success:
            # æ›´æ–°é…ç½®
            self.config["vector_database_type"] = "faiss"
            yield event.plain_result("âœ… æ•°æ®è¿ç§»æˆåŠŸï¼")
            yield event.plain_result("âš ï¸ è¯·é‡å¯æ’ä»¶ä»¥ä½¿ç”¨æ–°çš„ FAISS æ•°æ®åº“ã€‚")
        else:
            yield event.plain_result("âŒ æ•°æ®è¿ç§»å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚")

        # æ–­å¼€ç›®æ ‡æ•°æ®åº“è¿æ¥
        target_db.disconnect()

    except Exception as e:
        self.logger.error(f"è¿ç§»åˆ° FAISS å¤±è´¥: {e}", exc_info=True)
        yield event.plain_result(f"âš ï¸ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


async def migrate_to_milvus_cmd_impl(
    self: "Mnemosyne", event: AstrMessageEvent, confirm: Optional[str] = None
):
    """[å®ç°] è¿ç§»æ•°æ®åˆ° Milvus æ•°æ®åº“"""
    current_db_type = self.config.get("vector_database_type", "milvus")

    if current_db_type == "milvus":
        yield event.plain_result("âœ… å½“å‰å·²ç»ä½¿ç”¨ Milvus æ•°æ®åº“ï¼Œæ— éœ€è¿ç§»ã€‚")
        return

    if confirm != "--confirm":
        yield event.plain_result(
            f"âš ï¸ æ•°æ®åº“è¿ç§»ç¡®è®¤ âš ï¸\n"
            f"æ­¤æ“ä½œå°†æŠŠæ•°æ®ä» {current_db_type} è¿ç§»åˆ° Milvus æ•°æ®åº“ã€‚\n"
            f"è¯·ç¡®ä¿å·²æ­£ç¡®é…ç½® Milvus è¿æ¥ä¿¡æ¯ï¼š\n"
            f"- milvus_lite_path æˆ– address\n"
            f"- è®¤è¯ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰\n\n"
            f"å¦‚æœç¡®è®¤è¿ç§»ï¼Œè¯·æ‰§è¡Œï¼š\n"
            f"/memory migrate_to_milvus --confirm"
        )
        return

    try:
        yield event.plain_result("ğŸ”„ å¼€å§‹è¿ç§»åˆ° Milvus æ•°æ®åº“...")

        # æ£€æŸ¥å½“å‰æ•°æ®åº“è¿æ¥
        if not self.vector_db or not self.vector_db.is_connected():
            yield event.plain_result("âŒ å½“å‰æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•è¿›è¡Œè¿ç§»ã€‚")
            return

        # éªŒè¯ Milvus é…ç½®
        is_valid, error_msg = VectorDatabaseFactory.validate_config(
            "milvus", self.config
        )
        if not is_valid:
            yield event.plain_result(f"âŒ Milvus é…ç½®éªŒè¯å¤±è´¥: {error_msg}")
            return

        # åˆ›å»ºç›®æ ‡ Milvus æ•°æ®åº“
        yield event.plain_result("ğŸ“¦ åˆ›å»º Milvus æ•°æ®åº“å®ä¾‹...")
        target_db = VectorDatabaseFactory.create_database("milvus", self.config)
        if not target_db or not target_db.connect():
            yield event.plain_result("âŒ æ— æ³•åˆ›å»ºæˆ–è¿æ¥åˆ° Milvus æ•°æ®åº“ã€‚")
            return

        # æ‰§è¡Œæ•°æ®è¿ç§»
        yield event.plain_result(f"ğŸ“‹ å¼€å§‹è¿ç§»é›†åˆ '{self.collection_name}' çš„æ•°æ®...")

        # åœ¨åå°æ‰§è¡Œè¿ç§»
        success = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: VectorDatabaseFactory.migrate_data(
                source_db=self.vector_db,
                target_db=target_db,
                collection_name=self.collection_name,
                batch_size=1000,
            ),
        )

        if success:
            # æ›´æ–°é…ç½®
            self.config["vector_database_type"] = "milvus"
            yield event.plain_result("âœ… æ•°æ®è¿ç§»æˆåŠŸï¼")
            yield event.plain_result("âš ï¸ è¯·é‡å¯æ’ä»¶ä»¥ä½¿ç”¨æ–°çš„ Milvus æ•°æ®åº“ã€‚")
        else:
            yield event.plain_result("âŒ æ•°æ®è¿ç§»å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚")

        # æ–­å¼€ç›®æ ‡æ•°æ®åº“è¿æ¥
        target_db.disconnect()

    except Exception as e:
        self.logger.error(f"è¿ç§»åˆ° Milvus å¤±è´¥: {e}", exc_info=True)
        yield event.plain_result(f"âš ï¸ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


async def validate_config_cmd_impl(self: "Mnemosyne", event: AstrMessageEvent):
    """[å®ç°] éªŒè¯å½“å‰é…ç½®"""
    try:
        yield event.plain_result("ğŸ” å¼€å§‹éªŒè¯é…ç½®...")

        # éªŒè¯æ•°æ®åº“é…ç½®
        db_type = self.config.get("vector_database_type", "milvus")
        db_valid, db_error = VectorDatabaseFactory.validate_config(db_type, self.config)

        if db_valid:
            yield event.plain_result(f"âœ… {db_type} æ•°æ®åº“é…ç½®éªŒè¯é€šè¿‡")
        else:
            yield event.plain_result(f"âŒ {db_type} æ•°æ®åº“é…ç½®éªŒè¯å¤±è´¥: {db_error}")

        # éªŒè¯åµŒå…¥æœåŠ¡é…ç½®
        embedding_valid, embedding_error = EmbeddingServiceFactory.validate_config(
            self.config
        )

        if embedding_valid:
            yield event.plain_result("âœ… åµŒå…¥æœåŠ¡é…ç½®éªŒè¯é€šè¿‡")
        else:
            yield event.plain_result(f"âŒ åµŒå…¥æœåŠ¡é…ç½®éªŒè¯å¤±è´¥: {embedding_error}")

        # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
        required_fields = ["LLM_providers"]
        missing_fields = [
            field for field in required_fields if not self.config.get(field)
        ]

        if missing_fields:
            yield event.plain_result(f"âš ï¸ ç¼ºå°‘å¿…è¦é…ç½®: {', '.join(missing_fields)}")
        else:
            yield event.plain_result("âœ… å¿…è¦é…ç½®é¡¹æ£€æŸ¥é€šè¿‡")

        # æ€»ç»“
        all_valid = db_valid and embedding_valid and not missing_fields
        if all_valid:
            yield event.plain_result("\nğŸ‰ é…ç½®éªŒè¯å…¨éƒ¨é€šè¿‡ï¼æ’ä»¶åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
        else:
            yield event.plain_result("\nâš ï¸ å‘ç°é…ç½®é—®é¢˜ï¼Œè¯·æ ¹æ®ä¸Šè¿°æç¤ºè¿›è¡Œä¿®å¤ã€‚")

    except Exception as e:
        self.logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}", exc_info=True)
        yield event.plain_result(f"âš ï¸ é…ç½®éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


async def help_cmd_impl(self: "Mnemosyne", event: AstrMessageEvent):
    """[å®ç°] æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    # è·å–å½“å‰æ•°æ®åº“ç±»å‹ä»¥æä¾›æ›´å‡†ç¡®çš„å¸®åŠ©ä¿¡æ¯
    db_type = self.vector_db.get_database_type().value if self.vector_db else "å‘é‡æ•°æ®åº“"
    help_text = f"""ğŸ§  Mnemosyne é•¿æœŸè®°å¿†æ’ä»¶ v0.6.0
å½“å‰æ•°æ®åº“: {db_type}

ğŸ“‹ åŸºç¡€å‘½ä»¤:
  /memory status - æŸ¥çœ‹æ’ä»¶çŠ¶æ€å’Œé…ç½®ä¿¡æ¯
  /memory get_session_id - è·å–å½“å‰ä¼šè¯ID
  /memory validate_config - éªŒè¯å½“å‰é…ç½®

ğŸ“Š è®°å¿†ç®¡ç†:
  /memory list - åˆ—å‡ºæ‰€æœ‰é›†åˆ
  /memory list_records [é›†åˆå] [æ•°é‡] - æŸ¥çœ‹è®°å¿†è®°å½•
  /memory reset [--confirm] - æ¸…é™¤å½“å‰ä¼šè¯è®°å¿†

ğŸ”§ è¿ç§»å·¥å…· (ç®¡ç†å‘˜):
  /memory migrate_config - è¿ç§»é…ç½®åˆ°æ–°æ ¼å¼
  /memory migrate_to_faiss [--confirm] - è¿ç§»åˆ°FAISSæ•°æ®åº“
  /memory migrate_to_milvus [--confirm] - è¿ç§»åˆ°Milvusæ•°æ®åº“

ğŸ—‘ï¸ æ•°æ®ç®¡ç† (ç®¡ç†å‘˜):
  /memory drop_collection <é›†åˆå> [--confirm] - åˆ é™¤é›†åˆ
  /memory delete_session_memory <ä¼šè¯ID> [--confirm] - åˆ é™¤ä¼šè¯è®°å¿†

ğŸ’¡ ä½¿ç”¨æç¤º:
- æ–°ç”¨æˆ·æ¨èä½¿ç”¨ FAISS æ•°æ®åº“ï¼ˆç®€å•é«˜æ•ˆï¼‰
- ä¼ä¸šç”¨æˆ·å¯é€‰æ‹© Milvus æ•°æ®åº“
- è¿ç§»å‰å»ºè®®å…ˆæŸ¥çœ‹çŠ¶æ€ï¼š/memory status
- å±é™©æ“ä½œéœ€è¦æ·»åŠ  --confirm å‚æ•°ç¡®è®¤

ğŸ†• v0.6.0 æ–°åŠŸèƒ½:
âœ¨ æ”¯æŒå¤šç§å‘é‡æ•°æ®åº“ (Milvus + FAISS)
âœ¨ é›†æˆAstrBotåŸç”ŸåµŒå…¥æœåŠ¡
âœ¨ ä¸€é”®é…ç½®å’Œæ•°æ®è¿ç§»
âœ¨ æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—"""

    yield event.plain_result(help_text)
